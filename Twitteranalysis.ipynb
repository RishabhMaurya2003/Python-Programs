{"cells":[{"cell_type":"code","execution_count":1,"id":"ede41736","metadata":{"id":"ede41736","executionInfo":{"status":"ok","timestamp":1680584735098,"user_tz":-330,"elapsed":6320,"user":{"displayName":"MAURYA RISHABH SURESHCHANDRA Student","userId":"06460226496298257729"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sb\n","import nltk\n","\n","import re\n","\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk.stem import WordNetLemmatizer"]},{"cell_type":"code","execution_count":2,"id":"48e1cea8","metadata":{"id":"48e1cea8","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1680584735112,"user_tz":-330,"elapsed":24,"user":{"displayName":"MAURYA RISHABH SURESHCHANDRA Student","userId":"06460226496298257729"}},"outputId":"39fde475-d6f7-4659-af09-b88aa6218ee3"},"outputs":[{"output_type":"error","ename":"LookupError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-6bac02448a28>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"]}],"source":["stop_words = set(stopwords.words('english'))"]},{"cell_type":"code","execution_count":null,"id":"4b05aea6","metadata":{"id":"4b05aea6"},"outputs":[],"source":["df = pd.read_csv(\"Twitter_Data.csv\")"]},{"cell_type":"code","execution_count":null,"id":"329e9aba","metadata":{"scrolled":true,"id":"329e9aba","outputId":"75d8c6e9-80b6-42b4-f19c-931aba515f3b"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>clean_text</th>\n","      <th>category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>when modi promised â€œminimum government maximum...</td>\n","      <td>-1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>talk all the nonsense and continue all the dra...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>what did just say vote for modi  welcome bjp t...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>asking his supporters prefix chowkidar their n...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>answer who among these the most powerful world...</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                          clean_text  category\n","0  when modi promised â€œminimum government maximum...      -1.0\n","1  talk all the nonsense and continue all the dra...       0.0\n","2  what did just say vote for modi  welcome bjp t...       1.0\n","3  asking his supporters prefix chowkidar their n...       1.0\n","4  answer who among these the most powerful world...       1.0"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"id":"666686df","metadata":{"id":"666686df"},"outputs":[],"source":["df.drop(df[df['clean_text'].isna()].index, inplace=True)\n","df.drop(df[df['category'].isna()].index, inplace=True)"]},{"cell_type":"code","execution_count":null,"id":"75422eef","metadata":{"id":"75422eef"},"outputs":[],"source":["def preprocess_text(text):\n","    # Remove non-alphabetic characters and convert to lowercase\n","    text = re.sub('[^A-Za-z]', ' ', text.lower())\n","    \n","    # Tokenize the text\n","    tokens = text.split()\n","    \n","    # Remove stop words\n","    tokens = [word for word in tokens if word not in stop_words]\n","    \n","    # Perform stemming or lemmatization\n","    stemmer = PorterStemmer()\n","    lemmatizer = WordNetLemmatizer()\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","    tokens = [stemmer.stem(word) for word in tokens]\n","    \n","    # Join the tokens back into a string\n","    text = ' '.join(tokens)\n","    \n","    return text\n","\n","\n","def rem_unwanted(text):\n","    text_l = []\n","    for word in text.split(' '):\n","        if word.startswith('@') and len(word) > 1:\n","            word = ''\n","        \n","        if word.startswith('#') and len(word) > 1:\n","            word = ''\n","            \n","        elif word.startswith('http'):\n","            word = ''\n","        text_l.append(word)\n","\n","    return \" \".join(text_l)\n","\n","\n","#....................\n","import string\n","english_punctuations = string.punctuation\n","punctuations_list = english_punctuations\n","def punctuations_rem(text):\n","    translator = str.maketrans('', '', punctuations_list)\n","    return text.translate(translator)\n","\n","\n","#....................\n","def repeating_rem(text):\n","    return re.sub(r'(.)1+', r'1', text)"]},{"cell_type":"code","execution_count":null,"id":"b780159c","metadata":{"id":"b780159c"},"outputs":[],"source":["def final_preprocessing(text):\n","    text = emoji_rem(text)\n","    text = punctuations_rem(text)\n","    text = rem_unwanted(text)\n","    text = repeating_rem(text)\n","    text = preprocess_text(text)\n","    text = re.sub(r'[ \\n]+', ' ', text)\n","    \n","    return text"]},{"cell_type":"code","execution_count":null,"id":"f557f50b","metadata":{"id":"f557f50b"},"outputs":[],"source":["df['clean_text']= df['clean_text'].apply(lambda x: preprocess_text(x))"]},{"cell_type":"code","execution_count":null,"id":"e9189d49","metadata":{"id":"e9189d49"},"outputs":[],"source":["y = df['category']\n","X = df['clean_text']"]},{"cell_type":"code","execution_count":null,"id":"028e49d0","metadata":{"id":"028e49d0"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=121)"]},{"cell_type":"code","execution_count":null,"id":"bde982f6","metadata":{"id":"bde982f6"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score,hamming_loss,classification_report\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"]},{"cell_type":"code","execution_count":null,"id":"74e147fc","metadata":{"id":"74e147fc"},"outputs":[],"source":["tfidf = TfidfVectorizer()"]},{"cell_type":"code","execution_count":null,"id":"46e53580","metadata":{"id":"46e53580"},"outputs":[],"source":["pipe = Pipeline([('tfvec', TfidfVectorizer()), ('nb', RandomForestClassifier())])"]},{"cell_type":"code","execution_count":null,"id":"f46bbacf","metadata":{"id":"f46bbacf","outputId":"081b7243-5ed3-402a-e665-dcb4b35eb808"},"outputs":[{"data":{"text/plain":["Pipeline(steps=[('tfvec', TfidfVectorizer()), ('nb', RandomForestClassifier())])"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["pipe.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"id":"1cc7c87f","metadata":{"id":"1cc7c87f","outputId":"07e1a5b7-50eb-472d-e168-086ea421e5b2"},"outputs":[{"data":{"text/plain":["0.8052606819250987"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["pipe.score(X_test, y_test)"]},{"cell_type":"code","execution_count":null,"id":"65c3c9a9","metadata":{"id":"65c3c9a9","outputId":"4ba0b324-1a93-4584-fe7e-3c37f8c25dcd"},"outputs":[{"data":{"text/plain":["array([-1.,  1., -1., ...,  1.,  1.,  1.])"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["pipe.predict(X_test)"]},{"cell_type":"code","execution_count":null,"id":"8efe528c","metadata":{"id":"8efe528c"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[{"file_id":"1jzIH1zDpYaTJlrC2rCneKdXZGuYrpbZl","timestamp":1680584660531}]}},"nbformat":4,"nbformat_minor":5}